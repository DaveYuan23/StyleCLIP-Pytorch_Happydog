{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[]},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"pip install git+https://github.com/openai/CLIP.git","metadata":{"id":"48eef372-22dc-4591-b65d-e2fd6be55620","outputId":"ba9eb8fa-d240-4211-97d7-14dbb9c910ae","execution":{"iopub.status.busy":"2024-10-09T00:46:29.381820Z","iopub.execute_input":"2024-10-09T00:46:29.382239Z","iopub.status.idle":"2024-10-09T00:46:48.720742Z","shell.execute_reply.started":"2024-10-09T00:46:29.382202Z","shell.execute_reply":"2024-10-09T00:46:48.719400Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!wget https://github.com/ninja-build/ninja/releases/download/v1.8.2/ninja-linux.zip\n!sudo unzip ninja-linux.zip -d /usr/local/bin/\n!sudo update-alternatives --install /usr/bin/ninja ninja /usr/local/bin/ninja 1 --force","metadata":{"id":"MqOGrW1PPzEP","execution":{"iopub.status.busy":"2024-10-09T00:46:48.723268Z","iopub.execute_input":"2024-10-09T00:46:48.723654Z","iopub.status.idle":"2024-10-09T00:46:52.558178Z","shell.execute_reply.started":"2024-10-09T00:46:48.723616Z","shell.execute_reply":"2024-10-09T00:46:52.556951Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!git clone https://github.com/NVlabs/stylegan2-ada-pytorch.git\n%cd stylegan2-ada-pytorch","metadata":{"id":"390abce0-21f2-4139-8a54-84b306701304","outputId":"182ee258-c4ec-45e1-f52e-eed93f4e68c7","execution":{"iopub.status.busy":"2024-10-09T00:46:52.559693Z","iopub.execute_input":"2024-10-09T00:46:52.560020Z","iopub.status.idle":"2024-10-09T00:46:54.112756Z","shell.execute_reply.started":"2024-10-09T00:46:52.559986Z","shell.execute_reply":"2024-10-09T00:46:54.111278Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import torch\nimport pandas as pd\nimport numpy as np\nimport pickle\nimport os\nimport zipfile\nimport clip\n\nimport matplotlib.image as mpimg\nimport matplotlib.pyplot as plt\nfrom PIL import ImageEnhance, Image\nimport torchvision.utils as vutils\nfrom torchvision.transforms import ToPILImage","metadata":{"id":"9ba439ef-a877-4aca-866b-c9893ec061ed","execution":{"iopub.status.busy":"2024-10-09T00:46:54.115679Z","iopub.execute_input":"2024-10-09T00:46:54.116625Z","iopub.status.idle":"2024-10-09T00:46:56.200550Z","shell.execute_reply.started":"2024-10-09T00:46:54.116578Z","shell.execute_reply":"2024-10-09T00:46:56.199562Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"dataset_name='afhqdog' #@param ['ffhq'] {allow-input: true}\n# input dataset name \nos.makedirs('./model')\n\n# if not os.path.isfile('./model/'+dataset_name+'.pkl'):\n#         url='https://nvlabs-fi-cdn.nvidia.com/stylegan2/networks/'\n#         name='stylegan2-'+dataset_name+'-config-f.pkl'\n#         os.system('wget ' +url+name + '  -P  ./model/')\n#         os.system('mv ./model/'+name+' ./model/'+dataset_name+'.pkl')\nif not os.path.isfile('./model/'+dataset_name+'.pkl'):\n    url='https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/'\n    name=dataset_name+'.pkl'\n    os.system('wget ' +url+name + '  -P  ./model/ > /dev/null 2>&1')\n    os.system('mv ./model/'+name+' ./model/'+dataset_name+'.pkl > /dev/null 2>&1')\n","metadata":{"execution":{"iopub.status.busy":"2024-10-09T00:52:22.332438Z","iopub.execute_input":"2024-10-09T00:52:22.333121Z","iopub.status.idle":"2024-10-09T00:52:41.839192Z","shell.execute_reply.started":"2024-10-09T00:52:22.333063Z","shell.execute_reply":"2024-10-09T00:52:41.838175Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# load the pretrianed afhqdog model\nwith open('./model/afhqdog.pkl', 'rb') as f:\n    G = pickle.load(f)['G_ema']  # torch.nn.Module","metadata":{"id":"56ef34ca-f752-4da7-96ff-daa9dcb5f866","execution":{"iopub.status.busy":"2024-10-09T00:52:46.544592Z","iopub.execute_input":"2024-10-09T00:52:46.545010Z","iopub.status.idle":"2024-10-09T00:52:47.757338Z","shell.execute_reply.started":"2024-10-09T00:52:46.544969Z","shell.execute_reply":"2024-10-09T00:52:47.756005Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# you can also download other pretrained stylegan models from nvlabs!\n#!wget https://nvlabs-fi-cdn.nvidia.com/stylegan2-ada-pytorch/pretrained/metfaces.pkl -O metfaces.pkl","metadata":{"id":"0543b72a-59d9-472d-9d84-eb9f34f0966d","execution":{"iopub.status.busy":"2024-10-09T00:17:36.532312Z","iopub.execute_input":"2024-10-09T00:17:36.532962Z","iopub.status.idle":"2024-10-09T00:17:36.537148Z","shell.execute_reply.started":"2024-10-09T00:17:36.532921Z","shell.execute_reply":"2024-10-09T00:17:36.536187Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# transfer generator to cuda\ndevice = 'cuda'\nG = G.to(device)","metadata":{"id":"782d8f3e-4d29-4aa0-854b-e62fb9cc2230","outputId":"af186204-c768-4e46-bb9b-12f2d060ac59","execution":{"iopub.status.busy":"2024-10-09T00:17:43.625218Z","iopub.execute_input":"2024-10-09T00:17:43.626067Z","iopub.status.idle":"2024-10-09T00:17:43.632877Z","shell.execute_reply.started":"2024-10-09T00:17:43.626028Z","shell.execute_reply":"2024-10-09T00:17:43.631954Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#rm -rf ~/.cache/torch_extensions/*","metadata":{"id":"99855af8-9ada-4af1-bb73-b0ae00d30ab3","execution":{"iopub.status.busy":"2024-10-09T00:17:44.436601Z","iopub.execute_input":"2024-10-09T00:17:44.437001Z","iopub.status.idle":"2024-10-09T00:17:44.441846Z","shell.execute_reply.started":"2024-10-09T00:17:44.436963Z","shell.execute_reply":"2024-10-09T00:17:44.440575Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# generate a image from the stylegan generator and keep the lantent code\nseed = 9\ntorch.manual_seed(seed)  # set seed\nz = torch.randn([1, G.z_dim]).to(device).to(torch.float32)   # latent codes\nc = None                             # class labels (not used in this example)\nwith torch.no_grad():\n    w = G.mapping(z, c,truncation_psi=0.7)\n    img = G.synthesis(w)\n    #img = G(z,c)","metadata":{"id":"86aa1a23-d75c-424a-b332-f614df120894","execution":{"iopub.status.busy":"2024-10-09T00:30:50.379228Z","iopub.execute_input":"2024-10-09T00:30:50.380141Z","iopub.status.idle":"2024-10-09T00:30:50.414272Z","shell.execute_reply.started":"2024-10-09T00:30:50.380097Z","shell.execute_reply":"2024-10-09T00:30:50.413404Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# show images\ndef show_tensor_images(image_tensor, num_images = 16, size=(3, 64, 64), nrows = 4):\n    image_tensor = (image_tensor + 1)/2\n    image_unflat = image_tensor.detach().cpu().clamp_(0, 1)\n    image_grid = vutils.make_grid(image_unflat[:num_images], nrow = nrows, padding=0)\n    plt.imshow(image_grid.permute(1,2,0).squeeze())\n    plt.axis('off')\n    plt.show()","metadata":{"id":"06978826-9152-4f90-a509-3711f6320db5","execution":{"iopub.status.busy":"2024-10-09T00:30:50.572013Z","iopub.execute_input":"2024-10-09T00:30:50.572304Z","iopub.status.idle":"2024-10-09T00:30:50.578759Z","shell.execute_reply.started":"2024-10-09T00:30:50.572272Z","shell.execute_reply":"2024-10-09T00:30:50.577804Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_tensor_images(img, num_images=1,size=(3,512,512))","metadata":{"id":"f7a20ee0-d0fd-4ba1-94b2-eb9525d9ea11","outputId":"bc276edd-277c-46ab-c9e4-1ea0368ebbcf","execution":{"iopub.status.busy":"2024-10-09T00:30:50.789023Z","iopub.execute_input":"2024-10-09T00:30:50.789579Z","iopub.status.idle":"2024-10-09T00:30:51.006681Z","shell.execute_reply.started":"2024-10-09T00:30:50.789521Z","shell.execute_reply":"2024-10-09T00:30:51.005566Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"img.shape","metadata":{"id":"a6881ec4-d6d1-40e5-bf8f-d9283b8ef58d","outputId":"5cc61e60-0b82-421a-ee5f-0f8651622bff","execution":{"iopub.status.busy":"2024-10-09T00:30:55.929307Z","iopub.execute_input":"2024-10-09T00:30:55.930155Z","iopub.status.idle":"2024-10-09T00:30:55.936071Z","shell.execute_reply.started":"2024-10-09T00:30:55.930115Z","shell.execute_reply":"2024-10-09T00:30:55.935068Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# clip loss (calculate the similarity between generated images and the target text.)\nclass clip_loss(torch.nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.model, self.preprocess = clip.load(\"ViT-B/32\", device=\"cuda\")\n        self.upsample = torch.nn.Upsample(scale_factor=7)\n        self.avg_pool = torch.nn.AvgPool2d(kernel_size=16)\n\n    def forward(self, image, text):\n        image = self.avg_pool(self.upsample(image))\n        similarity = 1 - self.model(image, text)[0]/100\n        return similarity","metadata":{"id":"a9c5ee15-85b6-4d52-86e0-94f123e17fa7","execution":{"iopub.status.busy":"2024-10-09T00:30:56.116460Z","iopub.execute_input":"2024-10-09T00:30:56.116802Z","iopub.status.idle":"2024-10-09T00:30:56.123270Z","shell.execute_reply.started":"2024-10-09T00:30:56.116765Z","shell.execute_reply":"2024-10-09T00:30:56.122325Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"clipLoss = clip_loss()","metadata":{"id":"ee93202e-ea41-4ea8-bf77-24a836a426b6","execution":{"iopub.status.busy":"2024-10-09T00:30:56.377849Z","iopub.execute_input":"2024-10-09T00:30:56.378126Z","iopub.status.idle":"2024-10-09T00:31:00.725302Z","shell.execute_reply.started":"2024-10-09T00:30:56.378096Z","shell.execute_reply":"2024-10-09T00:31:00.724354Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# We can customize the specific editing we want.\ntext= 'A really happy dog face with mouth open' # [play with me] e.g. a relly sad face; a dog with blue eyes;\ntokenized_text = clip.tokenize([text]).to(device).long()\n\nlr_rampup = 0.05\nLR = 0.1\nepoch = 150\nl2_lambda = 0.0025\nsave_intermediate_image_every = 1\nresult_dir = 'results'","metadata":{"id":"6609046d-0dae-43e9-a447-5352e7fb423e","execution":{"iopub.status.busy":"2024-10-09T00:36:39.228671Z","iopub.execute_input":"2024-10-09T00:36:39.229092Z","iopub.status.idle":"2024-10-09T00:36:39.236262Z","shell.execute_reply.started":"2024-10-09T00:36:39.229042Z","shell.execute_reply":"2024-10-09T00:36:39.235245Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import os\nimport math\nimport torchvision\nfrom torch import optim\n\n\n# The learning rate adjustment function.\ndef get_lr(t, initial_lr, rampdown=0.50, rampup=0.05):\n    lr_ramp = min(1, (1 - t) / rampdown)\n    lr_ramp = 0.5 - 0.5 * math.cos(lr_ramp * math.pi)\n    lr_ramp = lr_ramp * min(1, t / rampup)\n\n    return initial_lr * lr_ramp\n\n\n\ntext_inputs = tokenized_text\nos.makedirs(result_dir, exist_ok=True)\n\n# Initialize the latent vector to be updated.\nw_star = w.detach().clone()\nw_star.requires_grad = True\n\nclipLoss = clip_loss()\noptimizer = torch.optim.Adam([w_star], LR)\n\nfor i in range(epoch):\n    # Adjust the learning rate.\n    t = (i+1) / epoch\n    lr = get_lr(t,LR)\n    optimizer.param_groups[0][\"lr\"] = lr\n\n    # Generate an image using the latent vector.\n    img_gen= G.synthesis(w_star)\n\n    # Calculate the loss value.\n    c_loss = clipLoss(img_gen, text_inputs)\n    l2_loss = ((w - w_star) ** 2).sum()\n    loss = c_loss + l2_lambda * l2_loss\n    # Get gradient and update the latent vector.\n    optimizer.zero_grad()\n    loss.backward()\n    optimizer.step()\n\n# Log the current state.\n    print(f\"lr: {lr}, loss: {loss.item():.4f}\")\n    if save_intermediate_image_every > 0 and i % save_intermediate_image_every == 0:\n        with torch.no_grad():\n            img_gen = G.synthesis(w_star)\n        show_tensor_images(img_gen, num_images=1,size=(3,512,512))\n        torchvision.utils.save_image(img_gen, f\"./results/{str(i).zfill(5)}.png\", normalize=True)\n\nwith torch.no_grad():\n    img_orig = G.synthesis(w, force_fp32=True)\n\n# Display the initial image and result image.\nfinal_result = torch.cat([img_orig, img_gen])\ntorchvision.utils.save_image(final_result.detach().cpu(), os.path.join(result_dir, \"final_result.jpg\"), normalize=True, scale_each=True)\n","metadata":{"scrolled":true,"id":"206705d8-0074-49b4-8e94-b07cec77e325","outputId":"2429d277-54fc-4983-d9cf-af3209e10bfa","execution":{"iopub.status.busy":"2024-10-09T00:36:41.726729Z","iopub.execute_input":"2024-10-09T00:36:41.727463Z","iopub.status.idle":"2024-10-09T00:38:10.843685Z","shell.execute_reply.started":"2024-10-09T00:36:41.727422Z","shell.execute_reply":"2024-10-09T00:38:10.842610Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_tensor_images(img_gen, num_images=1,size=(3,512,512))","metadata":{"id":"e65d0b9d-e2d0-4272-bbe2-f35299628bc9","outputId":"dd554037-8607-4a05-abcd-db9fdb9763ab","execution":{"iopub.status.busy":"2024-10-09T00:38:11.392195Z","iopub.execute_input":"2024-10-09T00:38:11.392732Z","iopub.status.idle":"2024-10-09T00:38:11.645342Z","shell.execute_reply.started":"2024-10-09T00:38:11.392688Z","shell.execute_reply":"2024-10-09T00:38:11.644418Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"show_tensor_images(img_orig, num_images=1,size=(3,512,512))","metadata":{"id":"7dd07dfe-91fd-4da4-9879-265e0e3038b8","outputId":"20d4ca19-0dd0-401d-8b55-f0a0f96d5cfe","execution":{"iopub.status.busy":"2024-10-09T00:38:12.647716Z","iopub.execute_input":"2024-10-09T00:38:12.648184Z","iopub.status.idle":"2024-10-09T00:38:12.903387Z","shell.execute_reply.started":"2024-10-09T00:38:12.648140Z","shell.execute_reply":"2024-10-09T00:38:12.902389Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# generate a video\n!ffmpeg -r 15 -i results/%05d.png -c:v libx264 -vf fps=25 -pix_fmt yuv420p out.mp4","metadata":{"id":"ecb9d175-b5fa-4fb2-a987-23c547b884e6","outputId":"02a987b8-b4c0-4fc2-b263-51a3cd1c1de4","execution":{"iopub.status.busy":"2024-10-09T00:38:21.752237Z","iopub.execute_input":"2024-10-09T00:38:21.752717Z","iopub.status.idle":"2024-10-09T00:38:36.102489Z","shell.execute_reply.started":"2024-10-09T00:38:21.752666Z","shell.execute_reply":"2024-10-09T00:38:36.101374Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# zip the output\nimport datetime\ndef file2zip(packagePath, zipPath):\n\n    zip = zipfile.ZipFile(zipPath, 'w', zipfile.ZIP_DEFLATED)\n    for path, dirNames, fileNames in os.walk(packagePath):\n        fpath = path.replace(packagePath, '')\n        for name in fileNames:\n            fullName = os.path.join(path, name)\n            name = fpath + '\\\\' + name\n            zip.write(fullName, name)\n    zip.close()\n\n\nif __name__ == \"__main__\":\n    # 文件夹路径\n    packagePath = './results'\n    zipPath = './output.zip'\n    if os.path.exists(zipPath):\n        os.remove(zipPath)\n    file2zip(packagePath, zipPath)\n    print(datetime.datetime.utcnow())","metadata":{"id":"3deaf9b7-85a8-4d78-8819-6cf56eec05d9","outputId":"93f96978-3152-482d-87f8-87d013355e39","execution":{"iopub.status.busy":"2024-10-09T00:38:36.104593Z","iopub.execute_input":"2024-10-09T00:38:36.104974Z","iopub.status.idle":"2024-10-09T00:38:38.860171Z","shell.execute_reply.started":"2024-10-09T00:38:36.104937Z","shell.execute_reply":"2024-10-09T00:38:38.859039Z"},"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{},"outputs":[],"execution_count":null}]}